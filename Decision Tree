## Step 1 - Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics

## Step 2 - Read in Libraries
fundraising = pd.read_csv(r"C:/Users/katey/Downloads/Fundraising.csv")
fundraising.info()

## Step 3 - Lambda Function
fundraising['Donor?'] = fundraising['TARGET_B'].apply(lambda x: 'Donor' if x == 1
                                                     else 'Non Donor')

fundraising.head()

## Step 4 - See the balance of incomes in the dataset
print(fundraising['INCOME'].value_counts())

## Step 5 - Create income ordinal for use in the average gift to income column calculation
fundraising['INCOME'] = fundraising['INCOME'].fillna(0)

fundraising['INCOME_ORDINAL'] = fundraising['INCOME'].astype(int)

## Step 6 - create average gift to income variable
fundraising['AVGGIFT_to_INC'] = fundraising['AVGGIFT'] / (fundraising['INCOME_ORDINAL']+1)

## Step 7 - Catagorize income levels to see if it has an impact on the tree
def categorize_income(val):
    if val in [1,2]:
        return 'Low'
    elif val in [3,4,5]:
        return 'Middle'
    else:
        return 'High'

## Step 8 - Continuation of catagorization
fundraising['income_category'] = fundraising['INCOME'].apply(categorize_income)

## Step 9 - See balance of Target B
y = fundraising['TARGET_B']
y.unique()

## Step 10 - Create decision tree with all columns included 
X = fundraising[['Row Id','Row Id.','zipconvert_2','zipconvert_3','zipconvert_4','zipconvert_5',
                'homeowner dummy','NUMCHLD','INCOME','gender dummy','WEALTH','HV','Icmed','Icavg',
                'IC15','NUMPROM','RAMNTALL','MAXRAMNT','LASTGIFT','totalmonths','TIMELAG','AVGGIFT',
                'INCOME_ORDINAL','AVGGIFT_to_INC']]

X_names = list(X.columns)

y_names = ['Donor','Non Donor']

print(X_names)
print(y_names)

## Step 11 - Split into training and testing for initial tree
X_train, X_test, y_train, y_test = train_test_split(X,
                                                   y,
                                                   test_size = 0.3,
                                                   random_state = 1,
                                                   stratify = fundraising['Donor?'])

## Step 12 - Upload decision tree classifier and set max depth for initial model
dt_clf = DecisionTreeClassifier(criterion = 'gini',
                               max_depth = 5,
                               random_state = 1)

dt_clf.fit(X_train,
          y_train)

## Step 13 - 
preds_train = dt_clf.predict(X_train)
preds_test = dt_clf.predict(X_test)

## Step 14 - Create initial tree
from sklearn import tree

fig = plt.figure(figsize = (10,8))

_ = tree.plot_tree(dt_clf,
             feature_names = X_names,
             class_names = y_names,
             filled = True);

## Step 15 - Find variable correlations to target B
dt_clf.feature_importances_

## Step 16 - Show confusion matrix metrics to see accuracy of initial model
metrics.confusion_matrix(y_true = y_train,
                        y_pred = preds_train)

## Step 17 - Drop columns that were not valued by the feature importances function
columns_to_drop = ['Row Id','Row Id.','zipconvert_2','zipconvert_3',
                 'zipconvert_4','zipconvert_5','homeowner dummy','NUMCHLD','INCOME','gender dummy',
                 'WEALTH','Icmed','Icavg','LASTGIFT','TIMELAG','AVGGIFT','INCOME_ORDINAL']

fundraising = fundraising.drop(columns=columns_to_drop)

fundraising.head()

## Step 18 - Define X and y variables for new model
X = fundraising[['HV','IC15','NUMPROM','RAMNTALL','MAXRAMNT','totalmonths', 'AVGGIFT_to_INC']]

X_names = list(X.columns)

y_names = ['Donor','Non Donor']

print(X_names)
print(y_names)

## Step 19 - Split for new model
X_train, X_test, y_train, y_test = train_test_split(X,
                                                   y,
                                                   test_size = 0.3,
                                                   random_state = 1,
                                                   stratify = fundraising['Donor?'])

## Step 20 - Set parameters for new model
dt_clf = DecisionTreeClassifier(criterion = 'gini',
                               max_depth = 5,
                               random_state = 1)

dt_clf.fit(X_train,
          y_train)

## Step 21 - Set training and testing data
preds_train = dt_clf.predict(X_train)
preds_test = dt_clf.predict(X_test)

## Step 22 - Plot new tree
from sklearn import tree

fig = plt.figure(figsize = (10,8))

_ = tree.plot_tree(dt_clf,
                  feature_names = X_names,
                  class_names = y_names,
                  filled = True);

## Step 23 - Print confusion matrix metrics to confirm increased viability
metrics.confusion_matrix(y_true = y_train,
                        y_pred = preds_train)

## Step 24 - Create confusion matrix for training data
from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_estimator(dt_clf,
                                     X_train,
                                     y_train,
                                     display_labels = y_names,
                                     cmap = 'Blues',
                                     colorbar = False);

## Step 25 - Print classification metrics for training data
print('--Classification Metrics: Training Data--')
print(metrics.classification_report(y_true = y_train,
                                   y_pred = preds_train))

## Step 26 - Print ROC curve for training data
from sklearn.metrics import roc_curve, auc
probs_test = dt_clf.predict_proba(X_train)
preds_tests = probs_test[:,1]

fpr,tpr,threshold = roc_curve(y_train,preds_train)
print(fpr)
print(tpr)
print(threshold)

## Step 27 - Continuation of ROC curve coding
roc_auc = auc(fpr,tpr)

import matplotlib.pyplot as plt
plt.plot(fpr,tpr,'b',label = 'AUC = %0.4F' % roc_auc)
plt.plot([0,1],[0,1],'r--')
plt.xlim([0,1])
plt.ylim([0,1])
plt.xlabel('True Positive Rate (TPR)')
plt.ylabel('False Positive Rate (FPR)')
plt.title('Receiver Operating Characteristics (ROC)')
plt.legend(loc = 'lower right')
plt.show();


## Step 28 - Print confusion matrix for testing data
fig, ax = plt.subplots(figsize = (8,4))

ConfusionMatrixDisplay.from_predictions(y_test,
                                       preds_test,
                                       display_labels = y_names,
                                       cmap = 'Blues',
                                       colorbar = False,
                                       ax=ax);

## Step 29 - Print classification metrics for testing data
print('--Classification Metrics: Testing Data--')
print(metrics.classification_report(y_true = y_test,
                                   y_pred = preds_test))

## Step 30 - Print ROC curve for testing data
from sklearn.metrics import roc_curve, auc
probs_test = dt_clf.predict_proba(X_test)
preds_tests = probs_test[:,1]

fpr,tpr,threshold = roc_curve(y_test,preds_test)
print(fpr)
print(tpr)
print(threshold)

## Step 31 - Continuation of ROC Curve printing
roc_auc = auc(fpr,tpr)

import matplotlib.pyplot as plt
plt.plot(fpr,tpr,'b',label = 'AUC = %0.4F' % roc_auc)
plt.plot([0,1],[0,1],'r--')
plt.xlim([0,1])
plt.ylim([0,1])
plt.xlabel('True Positive Rate (TPR)')
plt.ylabel('False Positive Rate (FPR)')
plt.title('Receiver Operating Characteristics (ROC)')
plt.legend(loc = 'lower right')
plt.show();
